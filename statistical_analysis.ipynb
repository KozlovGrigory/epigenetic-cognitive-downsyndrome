{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8311c952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 422802/422802 [06:18<00:00, 1117.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import data_preproc as dp\n",
    "import correlation_cognitive_methylation as ccm\n",
    "import division_into_classes as dic\n",
    "\n",
    "files = {'cpg_values': 'D:/unn/GSE52588/GSE52588_beta_fn.npz',\n",
    "         'cpg_names': 'D:/unn/GSE52588/GSE52588_beta_fn.npz', \n",
    "         'data_samples': 'D:/unn/GSE52588/GSE52588_samples.txt',\n",
    "         'cognitive_frame': 'D:/unn/GSE52588/DOWN_FENOTIPO_No4,8,12_PerCorrelazioni.tsv',\n",
    "         'cpgs_annotations': 'D:/unn/GSE52588/cpgs_annotations.txt'} \n",
    "directory_research = 'D:/unn/down_syndrome_epigenetic'\n",
    "cognitive_indicator = 'go-nogo'\n",
    "alpha = 0.001\n",
    "\n",
    "methylation_frame, cognitive_frame, indicator_folder = dp.data_preproc(files, directory_research, cognitive_indicator)\n",
    "correlation_frame = ccm.correlation_cognitive_methylation(alpha, files, indicator_folder, cognitive_indicator, \n",
    "                                                      methylation_frame, cognitive_frame)\n",
    "\n",
    "classes_all_members, classes_unique_members = dic.division_into_classes (cognitive_indicator, cognitive_frame)\n",
    "print('Division into classes \\nall values:', *classes_all_members, '\\nunique values:', *classes_unique_members)\n",
    "division_method = int(input('\\nChoose how to divide into classes:\\n1. all values\\n2. unique values\\n3. manual division\\n'))\n",
    "if division_method == 3:\n",
    "    print(sorted(list(cognitive_frame[cognitive_indicator])))\n",
    "    lim1, lim2 = [int(i) for i in input('Enter boundary values: ').split(' ')]\n",
    "    indicator_classes = dic.division_into_classes(cognitive_indicator, cognitive_frame, division_method, lim1, lim2)\n",
    "else:\n",
    "    indicator_classes = dic.division_into_classes(cognitive_indicator, cognitive_frame, division_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_metrics (y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "    return acc, f1\n",
    "\n",
    "def Grid_Search_CV (model, params, X_train, X_test, \n",
    "                    y_train, y_test, acc_lst, f1_lst, \n",
    "                    best_params_lst, cv_err_lst):\n",
    "    grid_cv = GridSearchCV(estimator = model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    best_model, best_params, cv_err = grid_cv.best_estimator_, grid_cv.best_params_, grid_cv.best_score_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc, f1 = quality_metrics (y_test, y_pred)\n",
    "    acc_lst.append(acc)\n",
    "    f1_lst.append(f1)\n",
    "    best_params_lst.append(best_params)\n",
    "    cv_err_lst.append(cv_err)\n",
    "    \n",
    "    return best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst\n",
    "\n",
    "def ML_classification (correlation_frame, methylation_frame, indicator_classes, cognitive_indicator, indicator_folder):\n",
    "    X = methylation_frame[list(correlation_frame.index)]\n",
    "    y = indicator_classes['class']\n",
    "    X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111, stratify = y)\n",
    "    best_params_lst, cv_err_lst = [], []\n",
    "    acc_lst, f1_lst = [], []\n",
    "    \n",
    "    dtree_model = tree.DecisionTreeClassifier(random_state = 111)\n",
    "    params = {'max_depth': range (1, 4, 1)}\n",
    "    dtree_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(dtree_model, params, X_train, X_test, \n",
    "                                                                                    y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                    best_params_lst, cv_err_lst)\n",
    "    rf_model = RandomForestClassifier(random_state = 111)\n",
    "    params = {'n_estimators': range(10, 110, 10)}\n",
    "    rf_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(rf_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    xg_model = xgb.XGBClassifier(objective ='multi:softprob', random_state = 111)\n",
    "    params = {'max_depth': range (1, 4, 1), 'n_estimators': range(10, 110, 10), \n",
    "              'learning_rate': [0.1, 0.01, 0.05]}\n",
    "    xgb.set_config(verbosity = 0)\n",
    "    xg_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(xg_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    cb_model = cb.CatBoostClassifier(loss_function = 'MultiClass', random_state = 111)\n",
    "    params = {'iterations': [100, 150, 200], 'learning_rate': [0.03, 0.1], \n",
    "              'depth': [2, 3, 4], 'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "    cb_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(cb_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    \n",
    "    lda_model = LDA()\n",
    "    params = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "    lda_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(lda_model, params, X_train, X_test, \n",
    "                                                                                  y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                  best_params_lst, cv_err_lst)\n",
    "    qda_model = QDA()\n",
    "    params = [{'reg_param': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]}]\n",
    "    qda_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(qda_model, params, X_train, X_test, \n",
    "                                                                                  y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                  best_params_lst, cv_err_lst)\n",
    "    logist = LogisticRegression(solver = 'liblinear')\n",
    "    params = [{'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}]\n",
    "    logist_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(logist, params, X_train, X_test, \n",
    "                                                                                     y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                     best_params_lst, cv_err_lst)\n",
    "    svc_linear = svm.SVC(kernel = 'linear')\n",
    "    params = [{'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]}]\n",
    "    best_svc_linear, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_linear, params, X_train, X_test, \n",
    "                                                                                   y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                   best_params_lst, cv_err_lst)\n",
    "    svc_rbf = svm.SVC(kernel = 'rbf')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "              'C': [40, 50, 60, 70, 80, 90]}]\n",
    "    best_svc_rbf, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_rbf, params, X_train, X_test, \n",
    "                                                                                y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                best_params_lst, cv_err_lst)\n",
    "    svc_poly = svm.SVC(kernel = 'poly')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0], \n",
    "              'C': [0.001, 0.01, 0.1, 1.0, 2.0, 5.0]}]\n",
    "    best_svc_poly, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_poly, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    best_models = [dtree_best_model, rf_best_model, xg_best_model, cb_best_model, lda_best_model, \n",
    "                   qda_best_model, logist_best_model, best_svc_linear, best_svc_rbf, best_svc_poly]\n",
    "    models_names = ['Desicion Tree', 'Random Forest', 'XGBoost', 'Catboost', 'LDA', 'QDA', 'Logistic regression', \n",
    "                    'SVC (linear kernel)', 'SVC (rbf kernel)', 'SVC (poly kernel)']\n",
    "    \n",
    "    CL_models = open('{0}/CL_models.txt'.format(indicator_folder), 'w')\n",
    "    for i in range(0, len(models_names)):\n",
    "        shap_interpretation(best_models[i], models_names[i], X, X_train, directory_folder_ML)\n",
    "        text = str(models_names[i]) + '\\nbest params: ' + str(best_params_lst[i]) + '\\nCV error = ' + str(cv_err_lst[i]) + '\\nAccuracy = ' + str(acc_lst[i]) + '\\nf1 = ' + str(f1_lst[i]) + '\\n\\n'\n",
    "        CL_models.write(text)\n",
    "    CL_models.close()\n",
    "    return best_params_lst, cv_err_lst, acc_lst, f1_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = methylation_frame[correlation_frame.index].values\n",
    "y = list(indicator_classes['class'])\n",
    "\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111, stratify = y)\n",
    "\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "params = [{'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}]\n",
    "\n",
    "grid_cv = GridSearchCV(estimator = model, param_grid = params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "    \n",
    "best_model = grid_cv.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "best_params = grid_cv.best_params_\n",
    "cv_err = grid_cv.best_score_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "explainer = shap.KernelExplainer(best_model.predict, X)\n",
    "shap_values = explainer.shap_values(X)\n",
    "    \n",
    "indicator_folder_SHAP = '{0}/SHAP'.format(indicator_folder)\n",
    "if not os.path.isdir(indicator_folder_SHAP):\n",
    "        os.mkdir(indicator_folder_SHAP)\n",
    "\n",
    "fig = plt.gcf()\n",
    "x, y = 15, 12\n",
    "fig_inch = (x/2.54, y/2.54)\n",
    "fig, ax = plt.subplots(figsize = fig_inch)\n",
    "shap.summary_plot(shap_values, features = X, feature_names = correlation_frame.index, max_display = 50, show = False)\n",
    "ax = plt.gca()\n",
    "fig.savefig('{0}/beeswarm.png'.format(indicator_folder_SHAP), format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "k = 0\n",
    "for i in shap_values:\n",
    "    fig = plt.gcf()\n",
    "    shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[k], feature_names = correlation_frame.index)\n",
    "    fig.savefig('{0}/waterfall_legacy_{1}.png'.format(indicator_folder_SHAP, k), \n",
    "                format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "    k += 1\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea8337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fd5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47630ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
