{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74086be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn import tree, svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, LeaveOneOut \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from progress.bar import IncrementalBar\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed3d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cognitive_indicator = 'corsi'\n",
    "indicator_folder = \"D:/unn/down_syndrome_epigenetic/corsi\"\n",
    "alpha = 0.001\n",
    "\n",
    "correlation_frame = pd.read_csv(r\"D:\\unn\\down_syndrome_epigenetic\\corsi\\correlation_frame.txt\", sep = '\\t', index_col = 0)\n",
    "methylation_frame = pd.read_csv(r\"D:\\unn\\down_syndrome_epigenetic\\corsi\\methylation_frame.txt\", sep = '\\t', index_col = 0)\n",
    "cognitive_frame = pd.read_csv(r\"D:\\unn\\down_syndrome_epigenetic\\corsi\\cognitive_frame.txt\", sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a332603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_boundary_values (cognitive_indicator, cognitive_frame, class_values, lim1 = 0, lim2 = 0):\n",
    "    if lim1 == 0 and lim2 == 0:\n",
    "        splits = np.array_split(class_values, 3)\n",
    "        lim1 = splits[0][-1]\n",
    "        lim2 = splits[1][-1]\n",
    "    \n",
    "    classes = pd.DataFrame({'value': list(cognitive_frame[cognitive_indicator]), 'class': 0})\n",
    "    for i in range(len(classes['value'])):\n",
    "        if classes['value'].loc[i] <= lim1:\n",
    "            classes.loc[i, 'class'] = 0\n",
    "        elif classes['value'].loc[i] > lim1 and classes['value'].loc[i] <= lim2:\n",
    "            classes.loc[i, 'class'] = 1\n",
    "        else:\n",
    "            classes.loc[i, 'class'] = 2\n",
    "    classes_members = [list(classes['class']).count(0),\n",
    "                       list(classes['class']).count(1),\n",
    "                       list(classes['class']).count(2)]\n",
    "    return classes, classes_members\n",
    "\n",
    "def division_into_classes (cognitive_indicator, cognitive_frame, division_method = 0, lim1 = 0, lim2 = 0):\n",
    "    if division_method != 3:\n",
    "        class_values = sorted(list(cognitive_frame[cognitive_indicator]))\n",
    "        classes_all, classes_all_members = division_boundary_values(cognitive_indicator, cognitive_frame, class_values)\n",
    "        if division_method == 1:\n",
    "            return classes_all\n",
    "        class_values = sorted(list(set(cognitive_frame[cognitive_indicator])))\n",
    "        classes_unique, classes_unique_members = division_boundary_values(cognitive_indicator, cognitive_frame, class_values)\n",
    "        if division_method == 2:\n",
    "            return classes_unique \n",
    "        return classes_all_members, classes_unique_members\n",
    "    else:\n",
    "        class_values = sorted(list(cognitive_frame[cognitive_indicator]))\n",
    "        classes_user, classes_user_members = division_boundary_values(cognitive_indicator, cognitive_frame, class_values, lim1, lim2)\n",
    "        return classes_user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4aa1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division into classes \n",
      "all values: 14 8 6 \n",
      "unique values: 9 13 6\n",
      "\n",
      "Choose how to divide into classes:\n",
      "1. all values\n",
      "2. unique values\n",
      "3. manual division\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "classes_all_members, classes_unique_members = division_into_classes (cognitive_indicator, cognitive_frame)\n",
    "print('Division into classes \\nall values:', *classes_all_members, '\\nunique values:', *classes_unique_members)\n",
    "division_method = int(input('\\nChoose how to divide into classes:\\n1. all values\\n2. unique values\\n3. manual division\\n'))\n",
    "if division_method == 3:\n",
    "    print(sorted(list(cognitive_frame[cognitive_indicator])))\n",
    "    lim1, lim2 = [int(i) for i in input('Enter boundary values: ').split(' ')]\n",
    "    indicator_classes = division_into_classes(cognitive_indicator, cognitive_frame, division_method, lim1, lim2)\n",
    "else:\n",
    "    indicator_classes = division_into_classes(cognitive_indicator, cognitive_frame, division_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44a559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAP_interpretation (model, model_name, X, correlation_frame, indicator_folder):\n",
    "    indicator_folder_SHAP = '{0}/SHAP'.format(indicator_folder)\n",
    "    if not os.path.isdir(indicator_folder_SHAP):\n",
    "            os.mkdir(indicator_folder_SHAP)\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model.predict, X)\n",
    "    shap_values = explainer.shap_values(X)        \n",
    "    \n",
    "    x, y = 15, 12\n",
    "    fig_inch = (x/2.54, y/2.54)\n",
    "    fig = plt.figure()\n",
    "    shap.summary_plot(shap_values, features = X, feature_names = correlation_frame.index, show = False)\n",
    "    plt.gcf().set_size_inches(fig_inch)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.title(model_name)\n",
    "    fig.savefig('{0}/beeswarm.png'.format(indicator_folder_SHAP), format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "    for patient_id in len(shap_values):\n",
    "        fig = plt.figure()\n",
    "        shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[patient_id], \n",
    "                                               feature_names = correlation_frame.index, show = False)\n",
    "        plt.gcf().set_size_inches(fig_inch)\n",
    "        fig.patch.set_facecolor('white')\n",
    "        fig.savefig('{0}/waterfall_legacy_{1}.png'.format(indicator_folder_SHAP, patient_id), \n",
    "                        format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "        \n",
    "def confusion_matrices (X, y, best_models, models_names, cognitive_indicator, indicator_folder):            \n",
    "    for model_opt in best_models:\n",
    "        cv = LeaveOneOut()\n",
    "        pred_targets = np.array([])\n",
    "        act_targets = np.array([])    \n",
    "        for train_ix, test_ix in cv.split(X):\n",
    "            Xm_train, Xm_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "            ym_train, ym_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "            model_opt.fit(Xm_train, ym_train)    \n",
    "            pred_labels = model_opt.predict(Xm_test)\n",
    "            pred_targets = np.append(pred_targets, pred_labels)\n",
    "            act_targets = np.append(act_targets, ym_test)\n",
    "\n",
    "        acc_m = accuracy_score(act_targets, pred_targets)\n",
    "        f1_m = f1_score(act_targets, pred_targets, average = 'macro')\n",
    "        \n",
    "        x, y = 15, 12\n",
    "        fig_inch = (x/2.54, y/2.54)\n",
    "        fig, ax = plt.subplots(figsize = fig_inch)\n",
    "        cf_matrix = confusion_matrix(act_targets, pred_targets)\n",
    "        ax = sns.heatmap(cf_matrix, annot = True, cmap = 'coolwarm')     \n",
    "        ax.set_title('{0}\\nAccuracy = {1:0.3f}, F1 = {2:0.3f}'.format(models_names[best_models.index(model_opt)], acc_m, f1_m), fontdict = {'size':'14'})\n",
    "        ax.set_xlabel('Predicted', fontdict = {'size':'14'})\n",
    "        ax.set_ylabel('Actual', fontdict = {'size':'14'})\n",
    "        ax.tick_params(axis = 'both', which = 'major', labelsize = 12)\n",
    "        ax.xaxis.set_ticklabels(['Small','Medium', 'Large'])\n",
    "        ax.yaxis.set_ticklabels(['Small','Medium', 'Large'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('{0}/{1}_{2}_svg.svg'.format(indicator_folder, cognitive_indicator, models_names[best_models.index(model_opt)]), dpi = 300)\n",
    "        plt.savefig('{0}/{1}_{2}_png.png'.format(indicator_folder, cognitive_indicator, models_names[best_models.index(model_opt)]), dpi = 300)\n",
    "        plt.savefig('{0}/{1}_{2}_tif.tif'.format(indicator_folder, cognitive_indicator, models_names[best_models.index(model_opt)]), dpi = 300)\n",
    "        plt.close()\n",
    "\n",
    "def quality_metrics (y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "    return acc, f1\n",
    "\n",
    "def Grid_Search_CV (model, params, X_train, X_test, \n",
    "                    y_train, y_test, acc_lst, f1_lst, \n",
    "                    best_params_lst, cv_err_lst):\n",
    "    grid_cv = GridSearchCV(estimator = model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    y_pred = grid_cv.predict(X_test)\n",
    "    best_params, cv_err = grid_cv.best_params_, grid_cv.best_score_\n",
    "    \n",
    "    acc, f1 = quality_metrics (y_test, y_pred)\n",
    "    acc_lst.append(acc)\n",
    "    f1_lst.append(f1)\n",
    "    best_params_lst.append(best_params)\n",
    "    cv_err_lst.append(cv_err)\n",
    "    \n",
    "    return grid_cv, best_params_lst, cv_err_lst, acc_lst, f1_lst\n",
    "\n",
    "def ML_classification (correlation_frame, methylation_frame, indicator_classes, cognitive_indicator, indicator_folder):\n",
    "    indicator_folder = '{0}/ML_classification'.format(indicator_folder)\n",
    "    if not os.path.isdir(indicator_folder):\n",
    "            os.mkdir(indicator_folder)\n",
    "            \n",
    "    X = methylation_frame[correlation_frame.index].values\n",
    "    y = indicator_classes['class']\n",
    "    X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111, stratify = y)\n",
    "    best_params_lst, cv_err_lst = [], []\n",
    "    acc_lst, f1_lst = [], []\n",
    "    \n",
    "    dtree_model = tree.DecisionTreeClassifier(random_state = 111)\n",
    "    params = {'max_depth': range (1, 4, 1)}\n",
    "    dtree_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(dtree_model, params, X_train, X_test, \n",
    "                                                                                    y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                    best_params_lst, cv_err_lst)\n",
    "    rf_model = RandomForestClassifier(random_state = 111)\n",
    "    params = {'n_estimators': range(10, 110, 10)}\n",
    "    rf_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(rf_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    xg_model = xgb.XGBClassifier(objective ='multi:softprob', random_state = 111)\n",
    "    params = {'max_depth': range (1, 4, 1), 'n_estimators': range(10, 110, 10), \n",
    "              'learning_rate': [0.1, 0.01, 0.05]}\n",
    "    xgb.set_config(verbosity = 0)\n",
    "    xg_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(xg_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    cb_model = cb.CatBoostClassifier(loss_function = 'MultiClass', random_state = 111)\n",
    "    params = {'iterations': [100, 150, 200], 'learning_rate': [0.03, 0.1], \n",
    "              'depth': [2, 3, 4], 'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "    cb_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(cb_model, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    \n",
    "    lda_model = LDA()\n",
    "    params = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "    lda_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(lda_model, params, X_train, X_test, \n",
    "                                                                                  y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                  best_params_lst, cv_err_lst)\n",
    "    qda_model = QDA()\n",
    "    params = [{'reg_param': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]}]\n",
    "    qda_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(qda_model, params, X_train, X_test, \n",
    "                                                                                  y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                  best_params_lst, cv_err_lst)\n",
    "    logist = LogisticRegression(solver = 'liblinear')\n",
    "    params = [{'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}]\n",
    "    logist_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(logist, params, X_train, X_test, \n",
    "                                                                                     y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                     best_params_lst, cv_err_lst)\n",
    "    svc_linear = svm.SVC(kernel = 'linear')\n",
    "    params = [{'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]}]\n",
    "    best_svc_linear, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_linear, params, X_train, X_test, \n",
    "                                                                                   y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                   best_params_lst, cv_err_lst)\n",
    "    svc_rbf = svm.SVC(kernel = 'rbf')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "              'C': [40, 50, 60, 70, 80, 90]}]\n",
    "    best_svc_rbf, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_rbf, params, X_train, X_test, \n",
    "                                                                                y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                best_params_lst, cv_err_lst)\n",
    "    svc_poly = svm.SVC(kernel = 'poly')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0], \n",
    "              'C': [0.001, 0.01, 0.1, 1.0, 2.0, 5.0]}]\n",
    "    best_svc_poly, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_poly, params, X_train, X_test, \n",
    "                                                                                 y_train, y_test, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    best_models = [dtree_best_model, rf_best_model, xg_best_model, cb_best_model, lda_best_model, \n",
    "                   qda_best_model, logist_best_model, best_svc_linear, best_svc_rbf, best_svc_poly]\n",
    "    models_names = ['Desicion Tree', 'Random Forest', 'XGBoost', 'Catboost', 'LDA', 'QDA', 'Logistic regression', \n",
    "                    'SVC (linear kernel)', 'SVC (rbf kernel)', 'SVC (poly kernel)']\n",
    "    \n",
    "    CL_models = open('{0}/CL_models.txt'.format(indicator_folder), 'w')\n",
    "    for i in range(len(models_names)):\n",
    "        text = str(models_names[i]) + '\\nbest params: ' + str(best_params_lst[i]) + '\\nCV error = ' + str(cv_err_lst[i]) + '\\nAccuracy = ' + str(acc_lst[i]) + '\\nf1 = ' + str(f1_lst[i]) + '\\n\\n'\n",
    "        CL_models.write(text)\n",
    "    CL_models.close()\n",
    "    \n",
    "    confusion_matrices(X, y, best_models, models_names, cognitive_indicator, indicator_folder)\n",
    "    \n",
    "    best_model_ind = acc_lst.index(max(acc_lst))\n",
    "    SHAP_interpretation(best_models[best_model_ind], models_names[best_model_ind], X, correlation_frame, indicator_folder)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10923c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b97cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7431ba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57886887, 0.29787216, 0.01261925, ..., 0.01328135, 0.84875625,\n",
       "        0.05028677],\n",
       "       [0.67036015, 0.4262362 , 0.01785992, ..., 0.01384891, 0.8603866 ,\n",
       "        0.06821622],\n",
       "       [0.663158  , 0.40857026, 0.01922581, ..., 0.01674174, 0.7851931 ,\n",
       "        0.10607845],\n",
       "       ...,\n",
       "       [0.6345485 , 0.38285524, 0.01860993, ..., 0.0150003 , 0.80481136,\n",
       "        0.0924546 ],\n",
       "       [0.69237185, 0.42694873, 0.02001639, ..., 0.01390312, 0.7734368 ,\n",
       "        0.06477937],\n",
       "       [0.7124195 , 0.4515766 , 0.02068484, ..., 0.0158811 , 0.8110887 ,\n",
       "        0.08276413]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylation_frame[correlation_frame.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de274edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1fc8cb574d4c198ccd52636acdb676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 425.197x340.157 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = methylation_frame[correlation_frame.index].values\n",
    "y = indicator_classes['class']\n",
    "\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111, stratify = y)\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf', probability=True)\n",
    "params = {'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0],\n",
    "          'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "          'C': [40, 50, 60, 70, 80, 90]} \n",
    "\n",
    "grid_cv = GridSearchCV(estimator = model, param_grid = params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "    \n",
    "best_params = grid_cv.best_params_\n",
    "cv_err = grid_cv.best_score_\n",
    "y_pred = grid_cv.predict(X_test)\n",
    "  \n",
    "explainer = shap.KernelExplainer(grid_cv.predict, X_train)\n",
    "shap_values = explainer.shap_values(X)    \n",
    "indicator_folder_SHAP = '{0}/SHAP'.format(indicator_folder)\n",
    "if not os.path.isdir(indicator_folder_SHAP):\n",
    "        os.mkdir(indicator_folder_SHAP)\n",
    "    \n",
    "x, y = 15, 12\n",
    "fig_inch = (x/2.54, y/2.54)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, features = X, feature_names = correlation_frame.index, show = False)\n",
    "plt.gcf().set_size_inches(fig_inch)\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.savefig('{0}/beeswarm.png'.format(indicator_folder_SHAP), format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "for patient_number in range(len(shap_values)):\n",
    "    fig = plt.figure()\n",
    "    shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[patient_number], \n",
    "                                           feature_names = correlation_frame.index, show = False)\n",
    "    plt.gcf().set_size_inches(fig_inch)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.savefig('{0}/waterfall_legacy_{1}.png'.format(indicator_folder_SHAP, patient_number), \n",
    "                    format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bae7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c95b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140bfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d90759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#изменено\n",
    "\n",
    "def quality_metrics (y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "    return acc, f1\n",
    "\n",
    "def Grid_Search_CV (model, params, X, y, acc_lst, f1_lst, \n",
    "                    best_params_lst, cv_err_lst):\n",
    "    X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111, stratify = y)\n",
    "    rfecv = RFECV(estimator = model, scoring = \"accuracy\", cv = 5)\n",
    "    rfecv.fit(X, y)\n",
    "    rfecv.transform(X)\n",
    "    \n",
    "    grid_cv = GridSearchCV(estimator = model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    best_model, best_params, cv_err = grid_cv.best_estimator_, grid_cv.best_params_, grid_cv.best_score_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc, f1 = quality_metrics (y_test, y_pred)\n",
    "    acc_lst.append(acc)\n",
    "    f1_lst.append(f1)\n",
    "    best_params_lst.append(best_params)\n",
    "    cv_err_lst.append(cv_err)\n",
    "    \n",
    "    return best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst\n",
    "\n",
    "def ML_classification (correlation_frame, methylation_frame, indicator_classes, cognitive_indicator, indicator_folder):\n",
    "    X = methylation_frame[list(correlation_frame.index)]\n",
    "    y = indicator_classes['class']    \n",
    "    best_params_lst, cv_err_lst = [], []\n",
    "    acc_lst, f1_lst = [], []\n",
    "    \n",
    "    lda_model = LDA()\n",
    "    params = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "    lda_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(lda_model, params, X, y, acc_lst, f1_lst, \n",
    "                                                                                  best_params_lst, cv_err_lst)\n",
    "    logist = LogisticRegression(solver = 'liblinear')\n",
    "    params = [{'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}]\n",
    "    logist_best_model, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(logist, params, X, y, acc_lst, f1_lst, \n",
    "                                                                                     best_params_lst, cv_err_lst)\n",
    "    svc_linear = svm.SVC(kernel = 'linear')\n",
    "    params = [{'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]}]\n",
    "    best_svc_linear, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_linear, params, X, y, acc_lst, f1_lst, \n",
    "                                                                                   best_params_lst, cv_err_lst)\n",
    "    svc_rbf = svm.SVC(kernel = 'rbf')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "              'C': [40, 50, 60, 70, 80, 90]}]\n",
    "    best_svc_rbf, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_rbf, params, X, y, acc_lst, f1_lst, \n",
    "                                                                                best_params_lst, cv_err_lst)\n",
    "    svc_poly = svm.SVC(kernel = 'poly')\n",
    "    params = [{'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0], 'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0], \n",
    "              'C': [0.001, 0.01, 0.1, 1.0, 2.0, 5.0]}]\n",
    "    best_svc_poly, best_params_lst, cv_err_lst, acc_lst, f1_lst = Grid_Search_CV(svc_poly, params, X, y, acc_lst, f1_lst, \n",
    "                                                                                 best_params_lst, cv_err_lst)\n",
    "    best_models = [dtree_best_model, rf_best_model, xg_best_model, cb_best_model, lda_best_model, \n",
    "                   qda_best_model, logist_best_model, best_svc_linear, best_svc_rbf, best_svc_poly]\n",
    "    models_names = ['Desicion Tree', 'Random Forest', 'XGBoost', 'Catboost', 'LDA', 'QDA', 'Logistic regression', \n",
    "                    'SVC (linear kernel)', 'SVC (rbf kernel)', 'SVC (poly kernel)']\n",
    "    \n",
    "    CL_models2 = open('{0}/CL_models_2.txt'.format(indicator_folder), 'w')\n",
    "    for i in range(0, len(models_names)):\n",
    "        text = str(models_names[i]) + '\\nbest params: ' + str(best_params_lst[i]) + '\\nCV error = ' + str(cv_err_lst[i]) + '\\nAccuracy = ' + str(acc_lst[i]) + '\\nf1 = ' + str(f1_lst[i]) + '\\n\\n'\n",
    "        CL_models2.write(text)\n",
    "    CL_models2.close()\n",
    "    return best_params_lst, cv_err_lst, acc_lst, f1_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    " use_label_encoder = False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de272d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: \n",
      "best params: {'solver': 'svd'} \n",
      "CV error = 0.5900000000000001 \n",
      "Accuracy = 0.5 \n",
      "F = 0.3571428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 599, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 440, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\kozlo\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\linalg\\_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 16 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 599, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 440, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\kozlo\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\linalg\\_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 15 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "One or more of the test scores are non-finite: [0.59 0.31  nan]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = methylation_frame[list(correlation_frame.index)]\n",
    "y = indicator_classes['class']\n",
    "\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111)\n",
    "\n",
    "def Grid_Search_CV(model, params, X_train, X_test, y_train, y_test):\n",
    "    grid_cv = GridSearchCV(estimator = model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_cv.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    best_params = grid_cv.best_params_\n",
    "    cv_err = grid_cv.best_score_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    return best_model, best_params, cv_err, y_pred\n",
    "\n",
    "lda_model = LDA()\n",
    "params = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "lda_best_model, best_prams, cv_err, y_pred = Grid_Search_CV(lda_model, params, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print('LDA:', '\\nbest params:', best_prams, '\\nCV error =', cv_err, \n",
    "     '\\nAccuracy =', accuracy_score(y_test, y_pred), '\\nF =', f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb25c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: \n",
      "best params: {'solver': 'svd'} \n",
      "CV error = 0.5900000000000001 \n",
      "Accuracy = 0.5 \n",
      "F = 0.3571428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 599, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 440, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\kozlo\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\linalg\\_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 16 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 599, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 440, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\kozlo\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\linalg\\_decomp.py\", line 594, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 15 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "One or more of the test scores are non-finite: [0.59 0.31  nan]\n"
     ]
    }
   ],
   "source": [
    "X = methylation_frame[list(correlation_frame.index)]\n",
    "y = indicator_classes['class']\n",
    "\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111)\n",
    "\n",
    "def Grid_Search_CV(model, params, X_train, X_test, y_train, y_test):\n",
    "    grid_cv = GridSearchCV(estimator = model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid_cv.best_params_\n",
    "    cv_err = grid_cv.best_score_\n",
    "    y_pred = grid_cv.predict(X_test)\n",
    "    return grid_cv, best_params, cv_err, y_pred\n",
    "\n",
    "lda_model = LDA()\n",
    "params = [{'solver': ['svd', 'lsqr', 'eigen']}]\n",
    "lda_best_model, best_prams, cv_err, y_pred = Grid_Search_CV(lda_model, params, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print('LDA:', '\\nbest params:', best_prams, '\\nCV error =', cv_err, \n",
    "     '\\nAccuracy =', accuracy_score(y_test, y_pred), '\\nF =', f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73e8f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = methylation_frame[correlation_frame.index]\n",
    "y = indicator_classes['class']\n",
    "\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7408733a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg09931783</th>\n",
       "      <th>cg11111139</th>\n",
       "      <th>cg26330326</th>\n",
       "      <th>cg17394978</th>\n",
       "      <th>cg25603108</th>\n",
       "      <th>cg19636302</th>\n",
       "      <th>cg24257309</th>\n",
       "      <th>cg19127840</th>\n",
       "      <th>cg15567428</th>\n",
       "      <th>cg01444716</th>\n",
       "      <th>...</th>\n",
       "      <th>cg17155018</th>\n",
       "      <th>cg23054181</th>\n",
       "      <th>cg10536916</th>\n",
       "      <th>cg23836542</th>\n",
       "      <th>cg14111928</th>\n",
       "      <th>cg13372456</th>\n",
       "      <th>cg27306119</th>\n",
       "      <th>cg02291204</th>\n",
       "      <th>cg12026891</th>\n",
       "      <th>cg06381964</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>-1.239162</td>\n",
       "      <td>-2.136250</td>\n",
       "      <td>-1.883715</td>\n",
       "      <td>1.498007</td>\n",
       "      <td>-1.164528</td>\n",
       "      <td>-1.151469</td>\n",
       "      <td>-1.695273</td>\n",
       "      <td>-1.543058</td>\n",
       "      <td>-1.763648</td>\n",
       "      <td>-1.132743</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.230520</td>\n",
       "      <td>-1.259977</td>\n",
       "      <td>-1.029347</td>\n",
       "      <td>-1.247306</td>\n",
       "      <td>-1.171447</td>\n",
       "      <td>-1.417413</td>\n",
       "      <td>-1.557061</td>\n",
       "      <td>-0.952594</td>\n",
       "      <td>1.110417</td>\n",
       "      <td>-1.142206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0.945454</td>\n",
       "      <td>0.595562</td>\n",
       "      <td>0.579858</td>\n",
       "      <td>-0.537119</td>\n",
       "      <td>-0.318900</td>\n",
       "      <td>-0.143430</td>\n",
       "      <td>0.843432</td>\n",
       "      <td>-0.092269</td>\n",
       "      <td>0.366732</td>\n",
       "      <td>-0.346595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599839</td>\n",
       "      <td>-0.473808</td>\n",
       "      <td>-0.682123</td>\n",
       "      <td>-0.991565</td>\n",
       "      <td>-0.674472</td>\n",
       "      <td>-0.599953</td>\n",
       "      <td>-0.822237</td>\n",
       "      <td>-0.584715</td>\n",
       "      <td>1.369736</td>\n",
       "      <td>-0.562444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>0.773482</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>1.221944</td>\n",
       "      <td>-0.027221</td>\n",
       "      <td>0.422474</td>\n",
       "      <td>0.491565</td>\n",
       "      <td>0.700021</td>\n",
       "      <td>0.326213</td>\n",
       "      <td>0.665343</td>\n",
       "      <td>0.916805</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.601063</td>\n",
       "      <td>-0.050220</td>\n",
       "      <td>-0.229692</td>\n",
       "      <td>0.459748</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>-0.312193</td>\n",
       "      <td>0.043877</td>\n",
       "      <td>1.290339</td>\n",
       "      <td>-0.306840</td>\n",
       "      <td>0.661861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>-0.865499</td>\n",
       "      <td>-0.193973</td>\n",
       "      <td>0.455862</td>\n",
       "      <td>0.180966</td>\n",
       "      <td>0.025811</td>\n",
       "      <td>-0.008549</td>\n",
       "      <td>0.167094</td>\n",
       "      <td>-0.184905</td>\n",
       "      <td>1.405013</td>\n",
       "      <td>-0.329850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878700</td>\n",
       "      <td>-0.294023</td>\n",
       "      <td>-0.113379</td>\n",
       "      <td>-0.445306</td>\n",
       "      <td>0.126803</td>\n",
       "      <td>-0.112240</td>\n",
       "      <td>-0.129716</td>\n",
       "      <td>-0.632673</td>\n",
       "      <td>-0.612068</td>\n",
       "      <td>-0.243183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>-0.989098</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>-0.527548</td>\n",
       "      <td>0.640925</td>\n",
       "      <td>-0.815460</td>\n",
       "      <td>0.433345</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.827547</td>\n",
       "      <td>0.740666</td>\n",
       "      <td>0.406740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617211</td>\n",
       "      <td>0.066798</td>\n",
       "      <td>0.888249</td>\n",
       "      <td>1.343821</td>\n",
       "      <td>0.281493</td>\n",
       "      <td>2.117912</td>\n",
       "      <td>-0.688314</td>\n",
       "      <td>0.444174</td>\n",
       "      <td>-1.738490</td>\n",
       "      <td>1.983365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>-0.012435</td>\n",
       "      <td>1.052502</td>\n",
       "      <td>0.718767</td>\n",
       "      <td>-0.978269</td>\n",
       "      <td>-0.085130</td>\n",
       "      <td>0.387839</td>\n",
       "      <td>0.971762</td>\n",
       "      <td>0.659192</td>\n",
       "      <td>0.267899</td>\n",
       "      <td>0.717034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653539</td>\n",
       "      <td>0.878764</td>\n",
       "      <td>1.171320</td>\n",
       "      <td>1.803207</td>\n",
       "      <td>0.719587</td>\n",
       "      <td>0.848998</td>\n",
       "      <td>1.647331</td>\n",
       "      <td>3.140674</td>\n",
       "      <td>-1.662065</td>\n",
       "      <td>1.652625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1.823836</td>\n",
       "      <td>1.531925</td>\n",
       "      <td>0.176442</td>\n",
       "      <td>-0.608790</td>\n",
       "      <td>1.956859</td>\n",
       "      <td>0.874478</td>\n",
       "      <td>0.980085</td>\n",
       "      <td>0.977672</td>\n",
       "      <td>0.855768</td>\n",
       "      <td>1.642908</td>\n",
       "      <td>...</td>\n",
       "      <td>2.165394</td>\n",
       "      <td>1.440843</td>\n",
       "      <td>0.744293</td>\n",
       "      <td>0.855341</td>\n",
       "      <td>1.238755</td>\n",
       "      <td>0.515821</td>\n",
       "      <td>-0.270630</td>\n",
       "      <td>0.036039</td>\n",
       "      <td>0.058587</td>\n",
       "      <td>-0.014521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>-1.210127</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-1.256553</td>\n",
       "      <td>0.227720</td>\n",
       "      <td>-0.868722</td>\n",
       "      <td>-0.321731</td>\n",
       "      <td>0.441060</td>\n",
       "      <td>-0.821195</td>\n",
       "      <td>-1.033376</td>\n",
       "      <td>-0.896041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021637</td>\n",
       "      <td>-0.438659</td>\n",
       "      <td>-0.933450</td>\n",
       "      <td>-1.588068</td>\n",
       "      <td>-1.917403</td>\n",
       "      <td>-1.074200</td>\n",
       "      <td>-0.766740</td>\n",
       "      <td>-1.284220</td>\n",
       "      <td>1.075835</td>\n",
       "      <td>-1.122774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1.103252</td>\n",
       "      <td>1.065658</td>\n",
       "      <td>0.731131</td>\n",
       "      <td>-1.376989</td>\n",
       "      <td>2.035095</td>\n",
       "      <td>1.300874</td>\n",
       "      <td>0.934464</td>\n",
       "      <td>0.982985</td>\n",
       "      <td>2.261393</td>\n",
       "      <td>0.775767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382433</td>\n",
       "      <td>1.498639</td>\n",
       "      <td>3.665612</td>\n",
       "      <td>0.612079</td>\n",
       "      <td>0.960641</td>\n",
       "      <td>-0.380824</td>\n",
       "      <td>-0.502049</td>\n",
       "      <td>-0.933118</td>\n",
       "      <td>-0.188880</td>\n",
       "      <td>0.507153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-1.679513</td>\n",
       "      <td>-0.794613</td>\n",
       "      <td>-0.908550</td>\n",
       "      <td>1.150789</td>\n",
       "      <td>-0.749085</td>\n",
       "      <td>-0.476786</td>\n",
       "      <td>-1.019526</td>\n",
       "      <td>-0.836865</td>\n",
       "      <td>-1.230877</td>\n",
       "      <td>-0.550044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.826679</td>\n",
       "      <td>-0.387782</td>\n",
       "      <td>-0.316715</td>\n",
       "      <td>-0.256926</td>\n",
       "      <td>-1.293769</td>\n",
       "      <td>-0.344310</td>\n",
       "      <td>-0.933406</td>\n",
       "      <td>-1.039960</td>\n",
       "      <td>0.468657</td>\n",
       "      <td>-0.723130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>-0.284799</td>\n",
       "      <td>-1.059621</td>\n",
       "      <td>-1.585713</td>\n",
       "      <td>-0.732599</td>\n",
       "      <td>-0.966622</td>\n",
       "      <td>-0.911534</td>\n",
       "      <td>-0.619068</td>\n",
       "      <td>-1.637809</td>\n",
       "      <td>-0.995325</td>\n",
       "      <td>-0.844929</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331659</td>\n",
       "      <td>-1.514471</td>\n",
       "      <td>-0.738278</td>\n",
       "      <td>-1.405107</td>\n",
       "      <td>-1.052813</td>\n",
       "      <td>-0.579602</td>\n",
       "      <td>-1.659847</td>\n",
       "      <td>-0.152556</td>\n",
       "      <td>0.701086</td>\n",
       "      <td>-1.018396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>-1.400818</td>\n",
       "      <td>-0.926308</td>\n",
       "      <td>2.205870</td>\n",
       "      <td>-1.414912</td>\n",
       "      <td>-1.502923</td>\n",
       "      <td>0.234980</td>\n",
       "      <td>-1.447038</td>\n",
       "      <td>-0.867118</td>\n",
       "      <td>-1.800367</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.787907</td>\n",
       "      <td>-1.209074</td>\n",
       "      <td>-1.275682</td>\n",
       "      <td>-1.697230</td>\n",
       "      <td>-0.737022</td>\n",
       "      <td>-0.728461</td>\n",
       "      <td>-1.033827</td>\n",
       "      <td>-0.998634</td>\n",
       "      <td>1.456585</td>\n",
       "      <td>-0.910753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.844791</td>\n",
       "      <td>2.029523</td>\n",
       "      <td>0.392596</td>\n",
       "      <td>-1.584595</td>\n",
       "      <td>2.242438</td>\n",
       "      <td>1.717559</td>\n",
       "      <td>0.651033</td>\n",
       "      <td>2.110130</td>\n",
       "      <td>1.555549</td>\n",
       "      <td>0.920218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351414</td>\n",
       "      <td>3.242274</td>\n",
       "      <td>1.142158</td>\n",
       "      <td>1.485371</td>\n",
       "      <td>3.105110</td>\n",
       "      <td>0.533275</td>\n",
       "      <td>0.063536</td>\n",
       "      <td>0.281031</td>\n",
       "      <td>-1.900331</td>\n",
       "      <td>1.473669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.099231</td>\n",
       "      <td>0.466010</td>\n",
       "      <td>-1.038597</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.529350</td>\n",
       "      <td>0.218994</td>\n",
       "      <td>-0.422959</td>\n",
       "      <td>-0.421423</td>\n",
       "      <td>-0.322631</td>\n",
       "      <td>-0.556172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430775</td>\n",
       "      <td>0.022575</td>\n",
       "      <td>-0.573208</td>\n",
       "      <td>-0.435603</td>\n",
       "      <td>0.265468</td>\n",
       "      <td>-0.855357</td>\n",
       "      <td>-0.051943</td>\n",
       "      <td>0.226761</td>\n",
       "      <td>0.692363</td>\n",
       "      <td>-0.991359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.571489</td>\n",
       "      <td>0.544525</td>\n",
       "      <td>1.086467</td>\n",
       "      <td>-0.686033</td>\n",
       "      <td>0.201645</td>\n",
       "      <td>0.402491</td>\n",
       "      <td>1.322032</td>\n",
       "      <td>0.579395</td>\n",
       "      <td>1.570529</td>\n",
       "      <td>0.368785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206579</td>\n",
       "      <td>-0.346227</td>\n",
       "      <td>0.152078</td>\n",
       "      <td>0.557185</td>\n",
       "      <td>0.693571</td>\n",
       "      <td>-0.082379</td>\n",
       "      <td>0.116584</td>\n",
       "      <td>0.188617</td>\n",
       "      <td>-0.358263</td>\n",
       "      <td>0.931779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1.008006</td>\n",
       "      <td>0.474194</td>\n",
       "      <td>1.241421</td>\n",
       "      <td>-0.995953</td>\n",
       "      <td>-0.251193</td>\n",
       "      <td>0.695103</td>\n",
       "      <td>-0.366028</td>\n",
       "      <td>0.391139</td>\n",
       "      <td>-0.045411</td>\n",
       "      <td>0.967825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164266</td>\n",
       "      <td>0.620217</td>\n",
       "      <td>0.281729</td>\n",
       "      <td>1.086903</td>\n",
       "      <td>0.300239</td>\n",
       "      <td>2.842161</td>\n",
       "      <td>2.648012</td>\n",
       "      <td>0.822633</td>\n",
       "      <td>-0.972758</td>\n",
       "      <td>1.648179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>-1.551738</td>\n",
       "      <td>-0.395975</td>\n",
       "      <td>-0.917562</td>\n",
       "      <td>0.682875</td>\n",
       "      <td>-0.534392</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>-0.483113</td>\n",
       "      <td>-0.441769</td>\n",
       "      <td>-0.632950</td>\n",
       "      <td>-0.652779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237608</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>-0.416582</td>\n",
       "      <td>-0.032372</td>\n",
       "      <td>-0.175397</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>0.227541</td>\n",
       "      <td>-0.122520</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>-0.267433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0.119191</td>\n",
       "      <td>-0.097544</td>\n",
       "      <td>-0.293567</td>\n",
       "      <td>0.379166</td>\n",
       "      <td>0.733191</td>\n",
       "      <td>0.205750</td>\n",
       "      <td>0.161498</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>-0.339876</td>\n",
       "      <td>0.606665</td>\n",
       "      <td>...</td>\n",
       "      <td>1.283625</td>\n",
       "      <td>-0.335792</td>\n",
       "      <td>-0.489765</td>\n",
       "      <td>-0.382036</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>0.648914</td>\n",
       "      <td>0.801681</td>\n",
       "      <td>1.367057</td>\n",
       "      <td>0.101337</td>\n",
       "      <td>-0.706053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>-0.618311</td>\n",
       "      <td>-0.628438</td>\n",
       "      <td>-0.805520</td>\n",
       "      <td>-0.309085</td>\n",
       "      <td>-0.357936</td>\n",
       "      <td>-1.344620</td>\n",
       "      <td>-1.222943</td>\n",
       "      <td>-0.918438</td>\n",
       "      <td>-0.505303</td>\n",
       "      <td>0.356495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>-0.185943</td>\n",
       "      <td>-0.124947</td>\n",
       "      <td>-0.540040</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>-0.558470</td>\n",
       "      <td>-0.490144</td>\n",
       "      <td>-0.347579</td>\n",
       "      <td>0.427301</td>\n",
       "      <td>-0.275441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>0.463476</td>\n",
       "      <td>1.517505</td>\n",
       "      <td>0.716691</td>\n",
       "      <td>-1.587249</td>\n",
       "      <td>1.079353</td>\n",
       "      <td>1.737117</td>\n",
       "      <td>0.670902</td>\n",
       "      <td>1.963350</td>\n",
       "      <td>1.021127</td>\n",
       "      <td>2.310516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390149</td>\n",
       "      <td>1.154484</td>\n",
       "      <td>1.314562</td>\n",
       "      <td>1.745039</td>\n",
       "      <td>0.694413</td>\n",
       "      <td>1.530195</td>\n",
       "      <td>1.853181</td>\n",
       "      <td>1.334979</td>\n",
       "      <td>-2.423167</td>\n",
       "      <td>2.160864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-0.932644</td>\n",
       "      <td>-0.363764</td>\n",
       "      <td>-0.662924</td>\n",
       "      <td>1.197543</td>\n",
       "      <td>-1.058577</td>\n",
       "      <td>-1.610480</td>\n",
       "      <td>-1.317065</td>\n",
       "      <td>-0.887085</td>\n",
       "      <td>-0.687923</td>\n",
       "      <td>-0.612956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.896858</td>\n",
       "      <td>-0.324142</td>\n",
       "      <td>-0.762886</td>\n",
       "      <td>-0.460508</td>\n",
       "      <td>-0.206492</td>\n",
       "      <td>-1.257711</td>\n",
       "      <td>-0.204541</td>\n",
       "      <td>-1.560851</td>\n",
       "      <td>0.497968</td>\n",
       "      <td>-0.190828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>-0.226874</td>\n",
       "      <td>-0.671609</td>\n",
       "      <td>-0.320649</td>\n",
       "      <td>1.162835</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>-0.956759</td>\n",
       "      <td>-0.730337</td>\n",
       "      <td>-0.396234</td>\n",
       "      <td>-0.458198</td>\n",
       "      <td>-1.175284</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028277</td>\n",
       "      <td>-0.536470</td>\n",
       "      <td>-0.122689</td>\n",
       "      <td>0.126085</td>\n",
       "      <td>-0.253116</td>\n",
       "      <td>-0.533907</td>\n",
       "      <td>-0.897060</td>\n",
       "      <td>-0.419825</td>\n",
       "      <td>0.424610</td>\n",
       "      <td>-0.626130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>-0.214092</td>\n",
       "      <td>-0.611155</td>\n",
       "      <td>-0.652511</td>\n",
       "      <td>1.478678</td>\n",
       "      <td>-0.637998</td>\n",
       "      <td>-1.432702</td>\n",
       "      <td>-1.854264</td>\n",
       "      <td>-0.839204</td>\n",
       "      <td>-1.468836</td>\n",
       "      <td>-1.606049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578755</td>\n",
       "      <td>-0.925718</td>\n",
       "      <td>-0.648504</td>\n",
       "      <td>-0.866023</td>\n",
       "      <td>-1.317989</td>\n",
       "      <td>0.307755</td>\n",
       "      <td>-0.101547</td>\n",
       "      <td>-0.863461</td>\n",
       "      <td>1.013901</td>\n",
       "      <td>-0.869028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-1.087183</td>\n",
       "      <td>-0.504194</td>\n",
       "      <td>-0.315409</td>\n",
       "      <td>0.381119</td>\n",
       "      <td>-0.383825</td>\n",
       "      <td>-0.393565</td>\n",
       "      <td>-1.870844</td>\n",
       "      <td>-0.570000</td>\n",
       "      <td>-0.610994</td>\n",
       "      <td>-0.737825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617261</td>\n",
       "      <td>-0.865007</td>\n",
       "      <td>-0.426183</td>\n",
       "      <td>-0.395678</td>\n",
       "      <td>-0.460625</td>\n",
       "      <td>-0.999079</td>\n",
       "      <td>-0.105141</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>0.519781</td>\n",
       "      <td>-0.745488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>-0.356072</td>\n",
       "      <td>-1.687296</td>\n",
       "      <td>0.340115</td>\n",
       "      <td>0.084687</td>\n",
       "      <td>-0.715142</td>\n",
       "      <td>-1.010915</td>\n",
       "      <td>0.249069</td>\n",
       "      <td>-0.342126</td>\n",
       "      <td>-0.348693</td>\n",
       "      <td>-0.768542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066816</td>\n",
       "      <td>-0.982794</td>\n",
       "      <td>-0.829096</td>\n",
       "      <td>-0.904237</td>\n",
       "      <td>-0.927526</td>\n",
       "      <td>-0.641036</td>\n",
       "      <td>0.337196</td>\n",
       "      <td>0.384326</td>\n",
       "      <td>0.482443</td>\n",
       "      <td>-0.066048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.090348</td>\n",
       "      <td>-0.327661</td>\n",
       "      <td>0.932427</td>\n",
       "      <td>-0.858002</td>\n",
       "      <td>1.476361</td>\n",
       "      <td>1.748628</td>\n",
       "      <td>0.505531</td>\n",
       "      <td>1.654032</td>\n",
       "      <td>-0.207045</td>\n",
       "      <td>1.016359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390419</td>\n",
       "      <td>0.238358</td>\n",
       "      <td>-0.147263</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>-0.197160</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.183499</td>\n",
       "      <td>0.161583</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>0.221323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1.471046</td>\n",
       "      <td>0.610726</td>\n",
       "      <td>1.593587</td>\n",
       "      <td>-0.008941</td>\n",
       "      <td>-0.079047</td>\n",
       "      <td>0.121750</td>\n",
       "      <td>0.923544</td>\n",
       "      <td>0.628161</td>\n",
       "      <td>0.272585</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303412</td>\n",
       "      <td>0.528611</td>\n",
       "      <td>-0.056633</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>0.334427</td>\n",
       "      <td>-0.300464</td>\n",
       "      <td>1.291275</td>\n",
       "      <td>-0.549581</td>\n",
       "      <td>-0.568968</td>\n",
       "      <td>-0.673577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1.949741</td>\n",
       "      <td>1.134850</td>\n",
       "      <td>1.907819</td>\n",
       "      <td>-0.980204</td>\n",
       "      <td>0.752795</td>\n",
       "      <td>0.934896</td>\n",
       "      <td>1.838352</td>\n",
       "      <td>0.191515</td>\n",
       "      <td>0.535603</td>\n",
       "      <td>0.268381</td>\n",
       "      <td>...</td>\n",
       "      <td>2.565366</td>\n",
       "      <td>0.283185</td>\n",
       "      <td>0.556421</td>\n",
       "      <td>0.602138</td>\n",
       "      <td>0.564460</td>\n",
       "      <td>0.369650</td>\n",
       "      <td>1.000488</td>\n",
       "      <td>0.732494</td>\n",
       "      <td>0.270550</td>\n",
       "      <td>-0.092026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 2070 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cg09931783  cg11111139  cg26330326  cg17394978  cg25603108  cg19636302  \\\n",
       "1001   -1.239162   -2.136250   -1.883715    1.498007   -1.164528   -1.151469   \n",
       "1003    0.945454    0.595562    0.579858   -0.537119   -0.318900   -0.143430   \n",
       "1007    0.773482    0.219600    1.221944   -0.027221    0.422474    0.491565   \n",
       "1009   -0.865499   -0.193973    0.455862    0.180966    0.025811   -0.008549   \n",
       "1010   -0.989098    0.006236   -0.527548    0.640925   -0.815460    0.433345   \n",
       "1011   -0.012435    1.052502    0.718767   -0.978269   -0.085130    0.387839   \n",
       "1013    1.823836    1.531925    0.176442   -0.608790    1.956859    0.874478   \n",
       "1020   -1.210127   -0.375903   -1.256553    0.227720   -0.868722   -0.321731   \n",
       "1023    1.103252    1.065658    0.731131   -1.376989    2.035095    1.300874   \n",
       "1028   -1.679513   -0.794613   -0.908550    1.150789   -0.749085   -0.476786   \n",
       "1036   -0.284799   -1.059621   -1.585713   -0.732599   -0.966622   -0.911534   \n",
       "1038    0.004202   -1.400818   -0.926308    2.205870   -1.414912   -1.502923   \n",
       "1042    0.844791    2.029523    0.392596   -1.584595    2.242438    1.717559   \n",
       "1052    0.099231    0.466010   -1.038597   -0.000131   -0.529350    0.218994   \n",
       "1002    0.571489    0.544525    1.086467   -0.686033    0.201645    0.402491   \n",
       "1005    1.008006    0.474194    1.241421   -0.995953   -0.251193    0.695103   \n",
       "1006   -1.551738   -0.395975   -0.917562    0.682875   -0.534392   -0.004925   \n",
       "1014    0.119191   -0.097544   -0.293567    0.379166    0.733191    0.205750   \n",
       "1016   -0.618311   -0.628438   -0.805520   -0.309085   -0.357936   -1.344620   \n",
       "1019    0.463476    1.517505    0.716691   -1.587249    1.079353    1.737117   \n",
       "1021   -0.932644   -0.363764   -0.662924    1.197543   -1.058577   -1.610480   \n",
       "1022   -0.226874   -0.671609   -0.320649    1.162835    0.004799   -0.956759   \n",
       "1026   -0.214092   -0.611155   -0.652511    1.478678   -0.637998   -1.432702   \n",
       "1029   -1.087183   -0.504194   -0.315409    0.381119   -0.383825   -0.393565   \n",
       "1030   -0.356072   -1.687296    0.340115    0.084687   -0.715142   -1.010915   \n",
       "1037    0.090348   -0.327661    0.932427   -0.858002    1.476361    1.748628   \n",
       "1043    1.471046    0.610726    1.593587   -0.008941   -0.079047    0.121750   \n",
       "1018    1.949741    1.134850    1.907819   -0.980204    0.752795    0.934896   \n",
       "\n",
       "      cg24257309  cg19127840  cg15567428  cg01444716  ...  cg17155018  \\\n",
       "1001   -1.695273   -1.543058   -1.763648   -1.132743  ...   -1.230520   \n",
       "1003    0.843432   -0.092269    0.366732   -0.346595  ...    0.599839   \n",
       "1007    0.700021    0.326213    0.665343    0.916805  ...   -0.601063   \n",
       "1009    0.167094   -0.184905    1.405013   -0.329850  ...   -0.878700   \n",
       "1010    0.006560    0.827547    0.740666    0.406740  ...   -0.617211   \n",
       "1011    0.971762    0.659192    0.267899    0.717034  ...   -0.653539   \n",
       "1013    0.980085    0.977672    0.855768    1.642908  ...    2.165394   \n",
       "1020    0.441060   -0.821195   -1.033376   -0.896041  ...    1.021637   \n",
       "1023    0.934464    0.982985    2.261393    0.775767  ...    0.382433   \n",
       "1028   -1.019526   -0.836865   -1.230877   -0.550044  ...   -0.826679   \n",
       "1036   -0.619068   -1.637809   -0.995325   -0.844929  ...   -1.331659   \n",
       "1038    0.234980   -1.447038   -0.867118   -1.800367  ...   -1.787907   \n",
       "1042    0.651033    2.110130    1.555549    0.920218  ...    0.351414   \n",
       "1052   -0.422959   -0.421423   -0.322631   -0.556172  ...   -0.430775   \n",
       "1002    1.322032    0.579395    1.570529    0.368785  ...    0.206579   \n",
       "1005   -0.366028    0.391139   -0.045411    0.967825  ...   -0.164266   \n",
       "1006   -0.483113   -0.441769   -0.632950   -0.652779  ...   -0.237608   \n",
       "1014    0.161498    0.088085   -0.339876    0.606665  ...    1.283625   \n",
       "1016   -1.222943   -0.918438   -0.505303    0.356495  ...    0.097440   \n",
       "1019    0.670902    1.963350    1.021127    2.310516  ...    0.390149   \n",
       "1021   -1.317065   -0.887085   -0.687923   -0.612956  ...   -0.896858   \n",
       "1022   -0.730337   -0.396234   -0.458198   -1.175284  ...    1.028277   \n",
       "1026   -1.854264   -0.839204   -1.468836   -1.606049  ...   -0.578755   \n",
       "1029   -1.870844   -0.570000   -0.610994   -0.737825  ...   -0.617261   \n",
       "1030    0.249069   -0.342126   -0.348693   -0.768542  ...    0.066816   \n",
       "1037    0.505531    1.654032   -0.207045    1.016359  ...    0.390419   \n",
       "1043    0.923544    0.628161    0.272585    0.735678  ...    0.303412   \n",
       "1018    1.838352    0.191515    0.535603    0.268381  ...    2.565366   \n",
       "\n",
       "      cg23054181  cg10536916  cg23836542  cg14111928  cg13372456  cg27306119  \\\n",
       "1001   -1.259977   -1.029347   -1.247306   -1.171447   -1.417413   -1.557061   \n",
       "1003   -0.473808   -0.682123   -0.991565   -0.674472   -0.599953   -0.822237   \n",
       "1007   -0.050220   -0.229692    0.459748    0.174275   -0.312193    0.043877   \n",
       "1009   -0.294023   -0.113379   -0.445306    0.126803   -0.112240   -0.129716   \n",
       "1010    0.066798    0.888249    1.343821    0.281493    2.117912   -0.688314   \n",
       "1011    0.878764    1.171320    1.803207    0.719587    0.848998    1.647331   \n",
       "1013    1.440843    0.744293    0.855341    1.238755    0.515821   -0.270630   \n",
       "1020   -0.438659   -0.933450   -1.588068   -1.917403   -1.074200   -0.766740   \n",
       "1023    1.498639    3.665612    0.612079    0.960641   -0.380824   -0.502049   \n",
       "1028   -0.387782   -0.316715   -0.256926   -1.293769   -0.344310   -0.933406   \n",
       "1036   -1.514471   -0.738278   -1.405107   -1.052813   -0.579602   -1.659847   \n",
       "1038   -1.209074   -1.275682   -1.697230   -0.737022   -0.728461   -1.033827   \n",
       "1042    3.242274    1.142158    1.485371    3.105110    0.533275    0.063536   \n",
       "1052    0.022575   -0.573208   -0.435603    0.265468   -0.855357   -0.051943   \n",
       "1002   -0.346227    0.152078    0.557185    0.693571   -0.082379    0.116584   \n",
       "1005    0.620217    0.281729    1.086903    0.300239    2.842161    2.648012   \n",
       "1006    0.155359   -0.416582   -0.032372   -0.175397    0.069522    0.227541   \n",
       "1014   -0.335792   -0.489765   -0.382036   -0.016276    0.648914    0.801681   \n",
       "1016   -0.185943   -0.124947   -0.540040    0.942263   -0.558470   -0.490144   \n",
       "1019    1.154484    1.314562    1.745039    0.694413    1.530195    1.853181   \n",
       "1021   -0.324142   -0.762886   -0.460508   -0.206492   -1.257711   -0.204541   \n",
       "1022   -0.536470   -0.122689    0.126085   -0.253116   -0.533907   -0.897060   \n",
       "1026   -0.925718   -0.648504   -0.866023   -1.317989    0.307755   -0.101547   \n",
       "1029   -0.865007   -0.426183   -0.395678   -0.460625   -0.999079   -0.105141   \n",
       "1030   -0.982794   -0.829096   -0.904237   -0.927526   -0.641036    0.337196   \n",
       "1037    0.238358   -0.147263    0.864280   -0.197160    0.993394    0.183499   \n",
       "1043    0.528611   -0.056633    0.106808    0.334427   -0.300464    1.291275   \n",
       "1018    0.283185    0.556421    0.602138    0.564460    0.369650    1.000488   \n",
       "\n",
       "      cg02291204  cg12026891  cg06381964  \n",
       "1001   -0.952594    1.110417   -1.142206  \n",
       "1003   -0.584715    1.369736   -0.562444  \n",
       "1007    1.290339   -0.306840    0.661861  \n",
       "1009   -0.632673   -0.612068   -0.243183  \n",
       "1010    0.444174   -1.738490    1.983365  \n",
       "1011    3.140674   -1.662065    1.652625  \n",
       "1013    0.036039    0.058587   -0.014521  \n",
       "1020   -1.284220    1.075835   -1.122774  \n",
       "1023   -0.933118   -0.188880    0.507153  \n",
       "1028   -1.039960    0.468657   -0.723130  \n",
       "1036   -0.152556    0.701086   -1.018396  \n",
       "1038   -0.998634    1.456585   -0.910753  \n",
       "1042    0.281031   -1.900331    1.473669  \n",
       "1052    0.226761    0.692363   -0.991359  \n",
       "1002    0.188617   -0.358263    0.931779  \n",
       "1005    0.822633   -0.972758    1.648179  \n",
       "1006   -0.122520   -0.069912   -0.267433  \n",
       "1014    1.367057    0.101337   -0.706053  \n",
       "1016   -0.347579    0.427301   -0.275441  \n",
       "1019    1.334979   -2.423167    2.160864  \n",
       "1021   -1.560851    0.497968   -0.190828  \n",
       "1022   -0.419825    0.424610   -0.626130  \n",
       "1026   -0.863461    1.013901   -0.869028  \n",
       "1029    0.031579    0.519781   -0.745488  \n",
       "1030    0.384326    0.482443   -0.066048  \n",
       "1037    0.161583    0.130585    0.221323  \n",
       "1043   -0.549581   -0.568968   -0.673577  \n",
       "1018    0.732494    0.270550   -0.092026  \n",
       "\n",
       "[28 rows x 2070 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74f18a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     0\n",
       "5     1\n",
       "6     2\n",
       "7     1\n",
       "8     1\n",
       "9     0\n",
       "10    1\n",
       "11    0\n",
       "12    1\n",
       "13    1\n",
       "14    2\n",
       "15    2\n",
       "16    0\n",
       "17    2\n",
       "18    0\n",
       "19    2\n",
       "20    0\n",
       "21    1\n",
       "22    0\n",
       "23    0\n",
       "24    1\n",
       "25    1\n",
       "26    1\n",
       "27    2\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26c6cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2aa29458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 0.8333333333333334 , f1 = 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "svc_rbf = svm.SVC(kernel = 'rbf', probability=True)\n",
    "params = {'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0],\n",
    "          'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "          'C': [40, 50, 60, 70, 80, 90]} \n",
    "\n",
    "grid_cv = GridSearchCV(estimator = model, param_grid = params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "    \n",
    "best_params = grid_cv.best_params_\n",
    "cv_err = grid_cv.best_score_\n",
    "rbf_pred = grid_cv.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, rbf_pred)\n",
    "f1_test = f1_score(y_test, rbf_pred, average = 'macro')\n",
    "print('\\nAccuracy =', acc_test, ', f1 =', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "280d1b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF kernel:\n",
      "\n",
      "CV error = 0.6\n",
      "\n",
      "SVC, R train = 0.0\n",
      "SVC, R test = 0.16666666666666666\n",
      "\n",
      "Accuracy = 0.8333333333333334 , f1 = 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "svc_rbf = svm.SVC(kernel = 'rbf', probability=True)\n",
    "\n",
    "params = {'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0],\n",
    "          'coef0': [0.0001, 0.001, 0.01, 0.1, 0.2, 1.0],\n",
    "          'C': [40, 50, 60, 70, 80, 90]} \n",
    "\n",
    "grid_cv = GridSearchCV(svc_rbf, param_grid = params, \n",
    "                       cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('RBF kernel:')\n",
    "print('\\nCV error =', grid_cv.best_score_)\n",
    "\n",
    "svc_rbf = svm.SVC(kernel = 'rbf', gamma = grid_cv.best_estimator_.gamma, coef0 = grid_cv.best_estimator_.coef0, \n",
    "                  C = grid_cv.best_estimator_.C)\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "rbf_pred = svc_rbf.predict(X_test)\n",
    "\n",
    "print('\\nSVC, R train =', np.mean(y_train != svc_rbf.predict(X_train)))\n",
    "print('SVC, R test =', np.mean(y_test != rbf_pred))\n",
    "\n",
    "acc_test = accuracy_score(y_test, rbf_pred)\n",
    "f1_test = f1_score(y_test, rbf_pred, average = 'macro')\n",
    "print('\\nAccuracy =', acc_test, ', f1 =', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61de1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but SVC was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ba690ce375441fb91c0892cf54759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "X does not have valid feature names, but SVC was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(grid_cv\u001b[38;5;241m.\u001b[39mpredict, X)\n\u001b[1;32m----> 2\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m      4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m\n\u001b[0;32m      5\u001b[0m fig_inch \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.54\u001b[39m, y\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.54\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\shap\\explainers\\_kernel.py:190\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    189\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 190\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    192\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\shap\\explainers\\_kernel.py:388\u001b[0m, in \u001b[0;36mKernel.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m phi_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD):\n\u001b[1;32m--> 388\u001b[0m     vphi, vphi_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     phi[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi\n\u001b[0;32m    390\u001b[0m     phi_var[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi_var\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\shap\\explainers\\_kernel.py:565\u001b[0m, in \u001b[0;36mKernel.solve\u001b[1;34m(self, fraction_evaluated, dim)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    564\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg\n\u001b[1;32m--> 565\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mLassoLarsIC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meyAdj_aug\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# use a fixed regularization coeffcient\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg)\u001b[38;5;241m.\u001b[39mfit(mask_aug, eyAdj_aug)\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_least_angle.py:2188\u001b[0m, in \u001b[0;36mLassoLarsIC.fit\u001b[1;34m(self, X, y, copy_X)\u001b[0m\n\u001b[0;32m   2182\u001b[0m X, y, Xmean, ymean, Xstd \u001b[38;5;241m=\u001b[39m LinearModel\u001b[38;5;241m.\u001b[39m_preprocess_data(\n\u001b[0;32m   2183\u001b[0m     X, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept, _normalize, copy_X\n\u001b[0;32m   2184\u001b[0m )\n\u001b[0;32m   2186\u001b[0m Gram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompute\n\u001b[1;32m-> 2188\u001b[0m alphas_, _, coef_path_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mlars_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2190\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_Gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlasso\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2203\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_least_angle.py:172\u001b[0m, in \u001b[0;36mlars_path\u001b[1;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Gram \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX cannot be None if Gram is not None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse lars_path_gram to avoid passing X and y.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lars_path_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_Gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_Gram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_n_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_least_angle.py:729\u001b[0m, in \u001b[0;36m_lars_path_solver\u001b[1;34m(X, y, Xy, Gram, n_samples, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[0;32m    724\u001b[0m     corr_eq_dir \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT[n_active:], eq_dir)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;66;03m# if huge number of features, this takes 50% of time, I\u001b[39;00m\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;66;03m# think could be avoided if we just update it using an\u001b[39;00m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# orthogonal (QR) decomposition of X\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m     corr_eq_dir \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGram\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_active\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_active\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleast_squares\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# Explicit rounding can be necessary to avoid `np.argmax(Cov)` yielding\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# unstable results because of rounding errors.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m np\u001b[38;5;241m.\u001b[39maround(corr_eq_dir, decimals\u001b[38;5;241m=\u001b[39mcov_precision, out\u001b[38;5;241m=\u001b[39mcorr_eq_dir)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(grid_cv.predict, X)\n",
    "shap_values = explainer.shap_values(X)    \n",
    "    \n",
    "x, y = 15, 12\n",
    "fig_inch = (x/2.54, y/2.54)\n",
    "fig = plt.figure()\n",
    "shap.summary_plot(shap_values, features = X, feature_names = correlation_frame.index, show = False)\n",
    "plt.gcf().set_size_inches(fig_inch)\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.savefig('{0}/beeswarm.png'.format(indicator_folder_SHAP), format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "for patient_number in shap_values:\n",
    "    fig = plt.figure()\n",
    "    shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[patient_number], \n",
    "                                           feature_names = correlation_frame.index, show = False)\n",
    "    plt.gcf().set_size_inches(fig_inch)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.savefig('{0}/waterfall_legacy_{1}.png'.format(indicator_folder_SHAP, patient_number), \n",
    "                    format = 'png', dpi = 300, bbox_inches = 'tight')\n",
    "    patient_number += 1\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b15bfb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.00304014,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.00199769,\n",
       "         0.        ,  0.0005502 ],\n",
       "       ...,\n",
       "       [-0.00273388,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.00302608,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c26de73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    value  class\n",
       "0     0.0      0\n",
       "1     3.0      1\n",
       "2     3.0      1\n",
       "3     3.0      1\n",
       "4     1.0      0\n",
       "5     3.0      1\n",
       "6     4.0      2\n",
       "7     2.0      1\n",
       "8     3.0      1\n",
       "9     0.0      0\n",
       "10    2.0      1\n",
       "11    0.0      0\n",
       "12    3.0      1\n",
       "13    2.0      1\n",
       "14    4.0      2\n",
       "15    5.0      2\n",
       "16    0.0      0\n",
       "17    4.0      2\n",
       "18    0.0      0\n",
       "19    5.0      2\n",
       "20    1.0      0\n",
       "21    2.0      1\n",
       "22    0.0      0\n",
       "23    0.0      0\n",
       "24    2.0      1\n",
       "25    3.0      1\n",
       "26    3.0      1\n",
       "27    4.0      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7fb972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class\n",
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "5       1\n",
       "6       2\n",
       "7       1\n",
       "8       1\n",
       "9       0\n",
       "10      1\n",
       "11      0\n",
       "12      1\n",
       "13      1\n",
       "14      2\n",
       "15      2\n",
       "16      0\n",
       "17      2\n",
       "18      0\n",
       "19      2\n",
       "20      0\n",
       "21      1\n",
       "22      0\n",
       "23      0\n",
       "24      1\n",
       "25      1\n",
       "26      1\n",
       "27      2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6b759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
